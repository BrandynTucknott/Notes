\documentclass{exam}

\usepackage{amsmath,amssymb,amsfonts,amsthm,dsfont}
\usepackage{lib/extra}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{enumitem}
\usepackage{bbm}
\usepackage{pgfplots}
\usepackage{fontenc}
\usepackage{float}

\pgfplotsset{compat=1.18}

\title{ECE 569 Midterm}
\author{Brandyn Tucknott}
\date{Due 31 October 2025}

\begin{document}
\maketitle

\begin{questions}
    % question 1
    \question Show that the following sets or functions are convex
    \begin{parts}
        % part a
        \part $\mathcal{S} = \cbrac{x\in\R^n : \norm{x}_1 + \norm{x}_2 \leq 1}$.
        \begin{proof}
            Let $x, y\in \mathcal{S}$ be arbitrary and $\theta\in [0, 1]$. Then
            \begin{align*}
                \norm{\theta x + (1 - \theta)y}_1 + \norm{\theta x + (1 - \theta)y}_2 &\leq \norm{\theta x}_1 + \norm{(1 - \theta)y}_1 + \norm{\theta x}_2 + \norm{(1 - \theta)y}_2 \\
                &= \theta\norm{x}_1 + (1 - \theta)\norm{y}_1 + \theta\norm{x}_2 + (1 - \theta)\norm{y}_2 \\
                &= \theta\paren{\norm{x}_1 + \norm{x}_2} + (1 - \theta)\paren{\norm{y}_1 + \norm{y}_2} \\
                &\leq \theta (1) + (1 - \theta)(1) = 1.
            \end{align*}
            We conclude that $\mathcal{S}$ is convex by definition.
        \end{proof}


        % part b
        \part $\mathcal{S} = \cbrac{A\in\S^n : z^TAz \geq 1, z\in\mathcal{C}}$, where $\mathcal{C}\subseteq \R^n$ (not necessarily convex).
        \begin{proof}
            Let $A, B\in \mathcal{S}, z\in\mathcal{C}$ be arbitrary with $\theta\in [0, 1]$. Then
            \begin{align*}
                z^T\paren{\theta A + (1 - \theta)B}z &= z^T\theta Az + z^T(1 - \theta)Bz \\
                &= \theta z^TAz + (1 - \theta)z^TBz \\
                &\geq \theta(1) + (1 - \theta)(1) = 1.
            \end{align*}
            We conclude that $\mathcal{S}$ is convex by definition.
        \end{proof}


        % part c
        \part $\mathcal{S} = \mathcal{C}_1 - \mathcal{C}_2$ where $\mathcal{C}_1, \mathcal{C}_2$ are convex sets.
        \begin{proof}
            Let $a, b\in \mathcal{S}, x_a, x_b\in\mathcal{C}_1, y_a, y_b\in\mathcal{C}_2$ be arbitrary and $\theta\in [0, 1]$. Then
            \begin{align*}
                \theta a + (1 - \theta)b &= \theta (x_a - y_a) + (1 - \theta)(x_b - y_b) \\
                &= \paren{\theta x_a + (1 - \theta)x_b} - \paren{\theta y_a + (1 - \theta)y_b} \\
                &= c_1 - c_2,
            \end{align*}
            where $c_1 = \theta x_a + (1 - \theta)x_b \in\mathcal{C}_1$ and $c_2 = \theta y_a + (1 - \theta)y_b\in\mathcal{C}_2$ by definition of convex sets. Then certainly 
            $c_1 - c_2\in\mathcal{S}$, thus $\mathcal{S}$ is convex.
        \end{proof}


        % part d
        \part $f(x) = \sum_{i = 1}^n \max{0, 1 - x_i}$.
        \begin{proof}
            Let $x,y\in\dom f$ and $\theta\in [0, 1]$. Then
            \begin{align*}
                f(\theta x + (1 - \theta)y) &= \sum_{i = 1}^n \max\cbrac{0, 1 - \paren{\theta x_i + (1 - \theta)y_i}} \\
                &= \sum_{i = 1}^n \frac{0 + \paren{1 - \paren{\theta x_i + (1 - \theta)y_i}} + \abs{\paren{1 - \paren{\theta x_i + (1 - \theta)y_i}} - 0}}{2} \\
                &= \sum_{i = 1}^n \frac{1 - \theta x_i - (1 - \theta)y_i + \abs{1 - \paren{\theta x_i - (1 - \theta)y_i}}}{2} \\
                &= \sum_{i = 1}^n \frac{\theta + (1 - \theta) - \theta x_i - (1 - \theta)y_i + \abs{\theta + (1 - \theta) - \theta x_i - (1 - \theta)y_i}}{2} \\
                &\leq \sum_{i = 1}^n \frac{\theta + (1 - \theta) - \theta x_i - (1 - \theta)y_i + |\theta - \theta x_i| + |(1 - \theta) - (1 - \theta)y_i|}{2} \\
                &= \sum_{i = 1}^n \frac{\theta - \theta x_i + \theta |1 - x_i| + (1 - \theta) - (1 - \theta)y_i + (1 - \theta)|1 - y_i|}{2} \\
                &= \theta\sum_{i = 1}^n \frac{1 - x_i + |1 - x_i|}{2} + (1 - \theta)\sum_{i = 1}^n \frac{1 - y_i + |1 - y_i|}{2} \\
                &= \theta\sum_{i = 1}^n \max\cbrac{0, 1 - x_i} + (1 - \theta)\sum_{i = 1}^n \max\cbrac{0, 1 - y_i} \\
                &= \theta f(x) + (1 - \theta)f(y).
            \end{align*}
            Since $f(\theta x + (1 - \theta)y) \leq \theta f(x) + (1 - \theta)f(y)$, we conclude that $f$ is convex.
        \end{proof}


        % part e
        \part $f(x, t) = -\log\paren{t - \norm{x}_2}$, where $\dom f = \cbrac{(x, t)\in \R^{n + 1} : \norm{x}_2 < t}$.
        \begin{proof}
            Let $h: \R_{>0}\to\R$ as $h(x) = -\log(x)$ and $g: \R^{n + 1}\to \R_{>0}$ as $g(x, t) = t - \norm{x}_2$. It is sufficient to Show
            that $h$ is convex (non-increasing) and $g$ is concave. First we consider $h$. We compute the first and second derivatives of $h$ to be
            \begin{align*}
                h'(x) &= -\frac{\log(e)}{x},
                h''(x)&= \frac{\log(e)}{x^2}.
            \end{align*}
            Since $h''(x) > 0$ for all $x\in\R_{>0}$, we conclude $h$ is convex. Furthermore, since $h'(x) < 0$ on the same domain, we know $h$ is non-increasing. Now consider $g$. We are done if we can show
            $-g$ is convex. Let $(x, t), (y, s)\in \dom g = \dom f, \theta\in[0, 1]$. Then
            \begin{align*}
                -g(\theta x + (1 - \theta)y, \theta t + (1 - \theta)s) &= \norm{\theta x + (1 - \theta)y}_2 - (\theta t + (1 - \theta)s) \\
                &\leq \theta\norm{x}_2 + (1 - \theta)\norm{y}_2 - \theta t - (1 - \theta)s \\
                &= -\theta (t - \norm{x}_2) - (1 - \theta)(s - \norm{y}_2) \\
                &= -\paren{\theta g(x, t) + (1 - \theta)g(y, s)}.
            \end{align*}
            Thus $-g$ is convex, or equivalently $g$ is concave. Since $h$ is convex (non-increasing) and $g$ concave, $f = h\circ g$ is convex.
        \end{proof}
    \end{parts}


    \newpage
    % question 2
    \question Answer the following questions and provide justifications.
    \begin{parts}
        % part a
        \part Is the following optimization problem convex?
        $$\underset{x\in\R^2}{\min} \frac{1}{2}(x_1^2 + x_2) \text{ s.t. } -10 \leq x_2 \leq 10, x_1 \geq 5.$$
        \begin{proof}
            First, we show that $f: \R^2\to\R$ defined as $f(x) = 1/2 (x_1^2 + x_2)$ is convex. We compute the gradient and hessian of $f$ to be
            \begin{align*}
                \nabla f &= (x_1, 1/2)^T, \\
                \nabla^2 f &= \paren{\begin{matrix}
                    1 & 0 \\
                    0 & 0
                \end{matrix}}.
            \end{align*}
            Now for arbitrary $x\in\R^2$, compute
            \begin{align*}
                x^T\nabla^2 f x &= (x_1, x_2) \paren{\begin{matrix}1 & 0 \\ 0 & 0\end{matrix}}(x_1, x_2)^T \\
                &= (x_1, 0)(x_1, x_2)^T \\
                &= x_1^2 \geq 0.
            \end{align*}
            Thus $\nabla^2 f \succeq 0$ and $f$ is convex. Since $f$ is convex and the optimization problem has convex constraints, 
            we conclude that we have a convex optimization problem.
        \end{proof}

        % part b
        \part Let $A, B\in \R^{n \times n}, x_0\in\R^n$. Is the following optimization problem convex?
        $$\underset{x\in\R^n}{\min} x^TA^TAx \text{ s.t. } x^TB^TBx \leq 50, \norm{x - x_0}\leq 10.$$
        \begin{proof}
            First, observe that
            $$x^TA^TAx = (Ax)^TAx = \norm{Ax}_2^2.$$
            We can then rewrite the optimization problem as
            $$\underset{x\in\R^n}{\min} \norm{Ax}_2^2 \text{ s.t. } \norm{Bx}_2^2 \leq 50, \norm{x - x_0}\leq 10.$$
            Since $\norm{Ax}_2^2$ is convex and all constraints are also convex, we conclude the given problem is a convex optimization problem.
        \end{proof} 


        % part c
        \part Let $A\in\R^{n\times n}, x_0\in\R^n$. Is the following optimization problem convex?
        $$\underset{x\in\R^n}{\min} x^TA^TAx \text{ s.t. } \norm{x - x_0}_2 \geq 10.$$
        \begin{proof}
            Although our given function is convex (just as in question 2.b), here our given constraint is the set of all points outside of the circle of radius 10 centered at $x_0$ in $\R^2$.
            That set is clearly not convex, so our given optimization problem is not convex.
        \end{proof}


        % part d
        \part Consider the following optimization problem:
        $$\underset{x_1, x_2}{\min} e^{x_1} + \frac{1}{2}(x_1 - x_2)^2 - x_1 - x_2 = f(x).$$
        Verify that the above problem is a convex optimization problem.
        \begin{proof}
            We begin by computing the gradient and hessian of $f$:
            \begin{align*}
                \nabla f &= (e^{x_1} + (x_1 - x_2) - 1, -(x_1 - x_2) - 1)^T \\
                \nabla^2 f &= \paren{\begin{matrix}
                    e^{x_1} + 1 & -1 \\
                    -1 & 1
                \end{matrix}}
            \end{align*}
            Now let $v\in\R^2$ be arbitrary. Then
            \begin{align*}
                v^T \nabla^2 f(x) v &= (v_1, v_2)\paren{\begin{matrix}e^{x_1} + 1 & -1 \\ -1 & 1\end{matrix}}(v_1, v_2)^T \\
                &= (v_1(e^{x_1} + 1) - v_2, v_2  -v_1)(v_1, v_2)^T \\
                &= v_1(v_1(e^{x_1} + 1) - v_2) + v_2(v_2 - v_1) \\
                &= v_1^2(e^{x_1} + 1) - 2v_1v_2 + v_2^2 \\
                &> v_1^2 - 2v_1v_2 + v_2^2 \text{ since } e^{x_1} + 1 > 0 \\
                &= (v_1 - v_2)^2 \geq 0.
            \end{align*}
            We conclude that $\nabla^2 f \succ 0$ and by extension $f$ is convex. Since $f$ has no constraints and is convex, we can say that
            the given optimization problem is convex.
        \end{proof}
    \end{parts}






    \newpage
    % question 3
    \question Let $b_\ell\in\R^n, \ell = 1, \hdots, m$ be fixed vectors. Consider the following optimization problem.
    $$\underset{x\in\R^n}{\min} \frac{1}{2}\sum_{\ell = 1}^m (x - b_\ell)^T(x - b_\ell) = f(x).$$
    \begin{parts}
        % part a
        \part Verify that the above problem is a convex optimizatino problem.
        \begin{proof}
            Since $f(x) = 1/2\sum_{\ell = 1}^m (x - b_\ell)^T(x - b_\ell) = 1/2 \sum_{\ell = 1}^m \norm{x - b_\ell}_2^2$, we know $f$ is convex (sum of positively weighted convex functions is convex).
        \end{proof}


        % part b
        \part Find the optimal solution to the above problem in closed form.
        \begin{proof}
            Since we have an unconstrained convex optimization problem, the optimal solution $x^*$ occurs at $\nabla f = 0$.
            \begin{align*}
                \nabla f &= 0 \\
                \frac{1}{2}\sum_{\ell = 1}^m (2x^* - 2b_\ell^T) &= 0 \\
                \sum_{\ell = 1}^m x^* - b_\ell &= 0 \\
                x^*m &= \sum_{\ell = 1}^m \\
                x^* &= \frac{1}{m} \sum_{\ell = 1}^m b_\ell.
            \end{align*}
        \end{proof}


        % part c
        \part Consider a robustified version of the problem. We focus on the setting where each $b_i$ is only an estimate of the true vector $\overline{b_\ell}$ such that
        $$\overline{b_\ell} \in\mathcal{B}(b_\ell, r) := \cbrac{\hat{b_\ell} : \norm{\hat{b_\ell} - b_\ell}_2 \leq r}$$
        and the following reobust optimization:
        \begin{equation}
        \min_{x\in\R^n}\max_{\ell = 1, \hdots, m, \hat{b_\ell} \in \mathcal{B}(b_\ell, r)} \frac{1}{2}\sum_{\ell = 1}^m (\norm{x}_2^2 - 2\hat{b_\ell^Tx}).
        \end{equation}
        Show that (1) is equivalent to the following convex optimization:
        $$\min_{x\in\R^n} \frac{1}{2}\sum_{\ell = 1}^m \paren{\norm{x - b_\ell}_2^2 + 2r\norm{x}_2}.$$
        \begin{proof}
            Call $f(x) = 1/2\sum_{\ell = 1}^m \norm{x}_2^2 - 2\hat{b}_\ell^Tx$ and $g(x) = 1/2\sum_{\ell = 1}^m \norm{x - b_\ell}_2^2 + 2r\norm{x}_2$. We begin by computing the gradients for each function, and observe that 
            we are done if we can find a mapping $F$ such that $F(x_f^*) = x_g^*$ where $x_f^*, x_g^*$ is an optimal solution to $f$ and $g$ respectively. We begin by computing the gradient of $f$.
            \begin{align*}
                \nabla f(x_f^*) &= 0 \\
                \frac{1}{2}\sum_{\ell = 1}^m 2x_f^* - 2\hat{b}_\ell &= 0 \\
                \sum_{\ell = 1}^m x_f^* - \hat{b}_\ell &= 0 \\
                mx_f^* - \sum_{\ell = 1}^m \hat{b}_\ell &= 0 \\
                x_f^* &= \frac{1}{m}\sum_{\ell = 1}^m \hat{b}_\ell.
            \end{align*}

            Now we compute the gradient of $g$.
            \begin{align*}
                \nabla g(x_g^*) &= 0 \\
                \frac{1}{2}\sum_{\ell = 1}^m 2(x_g^* - b_\ell) + 2r\frac{x_g^*}{\norm{x_g^*}_2} &= 0 \\
                \sum_{\ell = 1}^m x_g^* - b_\ell + r\frac{x_g^*}{\norm{x_g^*}_2} &= 0 \\
                x_g^* + \frac{rx_g^*}{\norm{x_g^*}_2} &= \frac{1}{m}\sum_{\ell = 1}^m b_\ell \\
                x_g^*\paren{1 + \frac{r}{\norm{x_g^*}_2}} &= \frac{1}{m}\sum_{\ell = 1}^m b_\ell
            \end{align*}
            for $x \neq 0$. This tells us there is some scalar $\alpha \geq 0$ for which $\alpha x_g^* = 1/m \sum b_\ell$. Using this, we continue to evaluate the gradient. Let $s = 1/m \sum_{\ell = 1}^m b_\ell$. Then
            \begin{align*}
                x_g^* + \frac{rx_g^*}{\norm{x_g^*}_2} &= \frac{1}{m}\sum_{\ell = 1}^m b_\ell \\
                \alpha \frac{1}{m}\sum_{\ell = 1}^m b_\ell + \frac{r\alpha\frac{1}{m}\sum_{\ell = 1}^mb_\ell}{\norm{\alpha \frac{1}{m}\sum_{\ell = 1}^mb_\ell}_2} &= \frac{1}{m}\sum_{\ell = 1}^mb_\ell \\
                \alpha s + \frac{r\alpha s}{\alpha \norm{s}_2} &= s \\
                \paren{\alpha + \frac{r}{\norm{s}_2}}s &= s,
            \end{align*}
            so $\alpha + r / \norm{s}_2 = 1$ and $\alpha = 1 - r / \norm{s}_2$, allowing us to conclude that
            $$\alpha x_g^* = s \longrightarrow x_g^* = \frac{s}{\alpha} = \frac{1}{\alpha m}\sum_{\ell = 1}^m b_\ell.$$

            Observe that since $\hat{b}_\ell \in B_r(b_\ell)$, there exists $u_\ell\in B_r(b_\ell)$ such that $\hat{b}_\ell - u_\ell = b_\ell$.
            Then we can write a function $F: \R^n \to \R_{\neq 0}^n$ defined as $F(x) = \frac{1}{\alpha}x - \frac{1}{m}\sum_{\ell = 1}^m u_\ell$.
            \begin{align*}
                F(x_f^*) &= \frac{1}{\alpha}x_f^* - \frac{1}{m}\sum_{\ell = 1}^m u_\ell \\
                &= \frac{1}{\alpha}\frac{1}{m}\sum_{\ell = 1}^m \hat{b}_\ell - \frac{1}{m}\sum_{\ell = 1}^m u_\ell \\
                &= \frac{1}{\alpha}\frac{1}{m}\sum_{\ell = 1}^m b_\ell + u_\ell - \frac{1}{m}\sum_{\ell = 1}^m u_\ell \\
                &= \frac{1}{\alpha}\frac{1}{m}\sum_{\ell = 1}^m b_\ell \\
                &= x_g^*.
            \end{align*}
            Since there exists a mapping from $x_f^*$ to $x_g^*$, the two given convex problems are equivalent (sidenote: $u_\ell$ is 
            guaranteed fixed since it depends on $b_\ell$ [fixed by assumption] and $\hat{b}_\ell$ [chosen first; fixed while choosing $x$]).
        \end{proof}
    \end{parts}






    \newpage
    % question 4
    \question Answer the following questions on constrained optimization problems.
    \begin{parts}
        % part a
        \part Consider the following optimization problem
        \begin{align*}
            \min_{x_1, x_2} & 2x_1 + \frac{1}{2}(x_2 - 6)^2 \\
            \text{s.t. } & x_1 + 2x_2 = 4.
        \end{align*}
        Write down the KKT conditions for the equality constrained problem. You may let $\lambda$ be the Lagrange multiplier. 
        Find a KKT point $(x_1^*, x_2^*, \lambda^*)$ to the problem.
        \begin{proof}
            Let $f(x) = 2x_1 + 1/2(x_2 - 6)^2$, $h(x) = x_1 + 2x_2 - 4$, and $\lambda$ a Lagrange multiplier. Then we require the following KKT conditions: 
            \begin{align*}
                \nabla f(x) + \lambda\nabla h(x) &= 0, \\
                h(x) &= 0.
            \end{align*}
            Observe that $h(x) = x_1 + 2x_2 - 4 = 0 \longrightarrow x_1 = 4 - 2x_2$. We can then compute the gradients of $f$ and $h$ to be
            $$\nabla f = \paren{\begin{matrix}2 \\ x_2 - 6\end{matrix}}, \nabla h = \paren{\begin{matrix} 1 \\ 2\end{matrix}},$$
            giving us a system of equations
            $$\nabla f(x) + \lambda\nabla h(x) = 0 \longrightarrow \begin{cases}
                2 + \lambda = 0 \\
                x_2 - 6 + 2\lambda = 0
            \end{cases}.$$
            From this system, we obtain $\lambda = -2, x_2 = 10$, from which we compute $x_1 = 4 - 2x_2 = -16$. 
            Thus a KKT point to the given problem is $(x_1^* = -16, x_2^* = 10, \lambda^* = -2)$.
        \end{proof}


        % part b
        \part Consider the optimization problem
        \begin{align*}
            \min_{x_1, x_2} & \frac{1}{2}(x_1 - 3)^2 + \frac{1}{2}(x_2 + 3)^2 \\
            \text{s.t. } & x_1 + 2x_2 \geq 0 \\
            & x_1^2 + x_2^2 \leq 1.
        \end{align*}
        Write down the KKT conditions for the above problem. You may let $\mu_1, \mu_2$ be the dual variables corresponding to
        the first and second inequality, respectively. Find a KKT point $(x_1^*, x_2^*, \mu_1^*, \mu_2^*)$ to the problem.
        \begin{proof}
            For notational ease, let
            \begin{align*}
                f(x) &= \frac{1}{2}(x_1 - 3)^2 + \frac{1}{2}(x_2 + 3)^2 \\
                g_1(x) &= -x_1 - 2x_2 \\
                g_2(x) &= x_1^2 + x_2^2 - 1.
            \end{align*}
            The KKT conditions for this problem are as follows:
            \begin{align*}
                \mu_1, \mu_2 &\geq 0 \\
                g_1(x), g_2(x) &\leq 0 \\
                \mu_1g_1(x), \mu_2g_2(x) &= 0 \\
                \nabla f(x) + \mu_1\nabla g_1(x) + \mu_2\nabla g_2(x) &= 0.
            \end{align*}

            Suppose we have that $g_1 = g_2 = 0$. The first equality $g_1 = 0$ allows us to derive a relationship between $x_1$ and $x_2$
            $$g_1(x) = 0 \longrightarrow -x_1 - 2x_2 = 0 \longrightarrow x_1 = -2x_2,$$
            and the second equation allows us to find values find them.
            \begin{align*}
                g_2(x) &= 0 \\
                x_1^2 + x_2^2 - 1 &= 0 \\
                (-2x_2)^2 + x_2^2 &= 1 \\
                5x_2^2 &= 1 \\
                x_2 = \pm\frac{\sqrt{5}}{5}.
            \end{align*}
            Let us consider $x_2 = -\sqrt{5} / 5$. Then we know $x_1, x_2$ to be $\frac{2\sqrt{5}}{5}$ and $-\frac{\sqrt{5}}{5}$ respectively.


            Next we compute the gradients for $f, g_1, g_2$ to be
            \begin{align*}
                \nabla f(x) &= \paren{\begin{matrix} x_1 - 3 \\ x_2 + 3\end{matrix}} \\
                \nabla g_1(x) &= \paren{\begin{matrix}-1 \\ -2 \end{matrix}} \\
                \nabla g_2(x) &= \paren{\begin{matrix}2x_1 \\ 2x_2 \end{matrix}}.
            \end{align*}

            Solving for the system of equations given by the last KKT condition
            \begin{align*}
                \nabla f(x) + \mu_1\nabla g_1(x) + \mu_2\nabla g_2(x) = 0 \longleftrightarrow \begin{cases}
                    (x_1 - 3) - \mu_1 + 2\mu_2x_1 = 0 \\
                    (x_2 + 3) - 2\mu_1 + 2\mu_2x_2 = 0
                \end{cases}
            \end{align*}
            yields $\mu_1 = 3/5, \mu_2 = (9\sqrt{5} - 5)/10$. This allows us to conclude a solution to the given KKT problem is
            $(x_1^* = 2\sqrt{5}/5, x_2^* = -\sqrt{5}/5, \mu_1^* = 3 / 5, \mu_2^* = (9\sqrt{5} - 5)/10)$.
        \end{proof}
    \end{parts}
\end{questions}



\end{document}